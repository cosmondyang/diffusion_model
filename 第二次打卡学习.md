# 学习train class-conditioned diffusion model 
## 最基础开始
'''python
pip install -q diffusers 
'''
先进行setup

之后进行import

'''python'''
import torch
import torchvision
from torch import nn
from torch.nn import functional as F
from torch.utils.data import DataLoader
from diffusers import DDPMScheduler, UNet2DModel
from matplotlib import pyplot as plt
from tqdm.auto import tqdm

device = 'mps' if torch.backends.mps.is_available() else 'cuda' if torch.cuda.is_available() else 'cpu'
print(f'Using device: {device}')
'''
上述引用了 diffusers 中的 ddpmscheduler和unet2dmodel
以及torch中的functional被称作F。
以及torch 中的nn。

下一步引用mnist
'''python 

# Load the dataset
dataset = torchvision.datasets.MNIST(root="mnist/", train=True, download=True, transform=torchvision.transforms.ToTensor())

# Feed it into a dataloader (batch size 8 here just for demo)
train_dataloader = DataLoader(dataset, batch_size=8, shuffle=True)

# View some examples
x, y = next(iter(train_dataloader))
print('Input shape:', x.shape)
print('Labels:', y)
plt.imshow(torchvision.utils.make_grid(x)[0], cmap='Greys');
'''
torchvision.dataset.MNIST 是load最经典的数据。
将上述数据取到一个dataloader。（dataloader就是一个框架取数据，有维度）
反复看例子。通过imshow。
此处注解
###chatgpt对于dataloader是何的解释。
在计算机编程和机器学习领域，"DataLoader" 是一个常见的概念，通常指的是用于加载和管理数据的工具或类。具体来说，DataLoader 通常用于以下情况：
数据加载： DataLoader 用于从数据源（如文件、数据库、网络等）中加载数据，并将数据转换为程序可以处理的格式，例如数组、张量或其他数据结构。
数据批处理： DataLoader 可以将数据划分为小批次（batch），以便进行批处理（batch processing）。批处理是一种常见的数据处理技术，它可以提高训练模型的效率，特别是在深度学习中。
数据增强： 在某些情况下，DataLoader 可以执行数据增强操作，例如图像旋转、翻转或裁剪，以增加数据的多样性。
数据迭代： DataLoader 可以提供一种逐批次迭代数据的方式，这对于模型的训练和评估非常有用。
在机器学习中，特别是深度学习，PyTorch 和 TensorFlow 等框架提供了内置的 DataLoader 类，用于加载和处理训练和测试数据。这些库的 DataLoader 类通常具有配置数据加载、批处理大小、数据增强和数据迭代的选项。
总之，DataLoader 是一个重要的工具，用于有效地处理和管理机器学习任务中的数据，以便训练和评估模型。它有助于简化数据加载和处理的复杂性，同时提高了代码的可维护性。

##创建一个class-conditioned unet
具体方法如下：
先建一个unet2dmodel标准的。
通过一个embedding layer，学习class和unet之间的关系。
将上述学到的东西作为一个多余的管道对于internal unet input with 
net_input=torch.cat((x,class_cond),1)
将上述多余管道接到unet中就可以了。

我们将class_emb_size 设为4之后，其实这个数可以为任意，1到10或者更多。
classconditionedunet将长成如下模样。
'''python'''
class ClassConditionedUnet(nn.Module):
  def __init__(self, num_classes=10, class_emb_size=4):
    super().__init__()
    
    # The embedding layer will map the class label to a vector of size class_emb_size
    self.class_emb = nn.Embedding(num_classes, class_emb_size)

    # Self.model is an unconditional UNet with extra input channels to accept the conditioning information (the class embedding)
    self.model = UNet2DModel(
        sample_size=28,           # the target image resolution
        in_channels=1 + class_emb_size, # Additional input channels for class cond.
        out_channels=1,           # the number of output channels
        layers_per_block=2,       # how many ResNet layers to use per UNet block
        block_out_channels=(32, 64, 64), 
        down_block_types=( 
            "DownBlock2D",        # a regular ResNet downsampling block
            "AttnDownBlock2D",    # a ResNet downsampling block with spatial self-attention
            "AttnDownBlock2D",
        ), 
                up_block_types=(
            "AttnUpBlock2D", 
            "AttnUpBlock2D",      # a ResNet upsampling block with spatial self-attention
            "UpBlock2D",          # a regular ResNet upsampling block
          ),
    )

  # Our forward method now takes the class labels as an additional argument
  def forward(self, x, t, class_labels):
    # Shape of x:
    bs, ch, w, h = x.shape
    
    # class conditioning in right shape to add as additional input channels
    class_cond = self.class_emb(class_labels) # Map to embedding dimension
    class_cond = class_cond.view(bs, class_cond.shape[1], 1, 1).expand(bs, class_cond.shape[1], w, h)
    # x is shape (bs, 1, 28, 28) and class_cond is now (bs, 4, 28, 28)

    # Net input is now x and class cond concatenated together along dimension 1
    net_input = torch.cat((x, class_cond), 1) # (bs, 5, 28, 28)

    # Feed this to the UNet alongside the timestep and return the prediction
    return self.model(net_input, t).sample # (bs, 1, 28, 28)

  '''

  这个classconditionedunet值得深入学习。把他搞懂。

  ##之后就是train和取样了。
  之前没有class的时候，我们predictby unet(x,y) 现在有了class后，我们通过了
  unet(x,t,y)一同进行training。
  我们现在也是进行noise的prediction从原先的DDPMscheduler中。

  先创建一个noise过的scheduler
  '''python
  # Create a scheduler
noise_scheduler = DDPMScheduler(num_train_timesteps=1000, beta_schedule='squaredcos_cap_v2')
'''python
#@markdown Training loop (10 Epochs):
# Redefining the dataloader to set the batch size higher than the demo of 8
train_dataloader = DataLoader(dataset, batch_size=128, shuffle=True)
# How many runs through the data should we do?
n_epochs = 10
# Our network 
net = ClassConditionedUnet().to(device)
# Our loss function
loss_fn = nn.MSELoss()
# The optimizer
opt = torch.optim.Adam(net.parameters(), lr=1e-3) 
# Keeping a record of the losses for later viewing
losses = []
# The training loop
for epoch in range(n_epochs):
    for x, y in tqdm(train_dataloader):
        # Get some data and prepare the corrupted version
        x = x.to(device) * 2 - 1 # Data on the GPU (mapped to (-1, 1))
        y = y.to(device)
        noise = torch.randn_like(x)
        timesteps = torch.randint(0, 999, (x.shape[0],)).long().to(device)
        noisy_x = noise_scheduler.add_noise(x, noise, timesteps)

        # Get the model prediction
        pred = net(noisy_x, timesteps, y) # Note that we pass in the labels y

        # Calculate the loss
        loss = loss_fn(pred, noise) # How close is the output to the noise

        # Backprop and update the params:
        opt.zero_grad()
        loss.backward()
        opt.step()

        # Store the loss for later
        losses.append(loss.item())
    # Print out the average of the last 100 loss values to get an idea of progress:
    avg_loss = sum(losses[-100:])/100
    print(f'Finished epoch {epoch}. Average of the last 100 loss values: {avg_loss:05f}')
# View the loss curve
plt.plot(losses)
'''

##chatgpt对于finetune代码的解释
以下是一段添加噪音和去除噪音的finetune代码，，我将用chatgpt对之进行一个解释。
'''python'''
import wandb
import numpy as np
import torch, torchvision
import torch.nn.functional as F
from PIL import Image
from tqdm.auto import tqdm
from fastcore.script import call_parse
from torchvision import transforms
from diffusers import DDPMPipeline
from diffusers import DDIMScheduler
from datasets import load_dataset
from matplotlib import pyplot as plt

@call_parse
def train(
    image_size = 256,
    batch_size = 16,
    grad_accumulation_steps = 2,
    num_epochs = 1,
    start_model = "google/ddpm-bedroom-256",
    dataset_name = "huggan/wikiart",
    device = 'cuda',
    model_save_name = 'wikiart_1e',
    wandb_project = 'dm_finetune',
    log_samples_every = 250,
    save_model_every = 2500,
    ):
        
    # Initialize wandb for logging
    wandb.init(project=wandb_project, config=locals())


    # Prepare pretrained model
    image_pipe = DDPMPipeline.from_pretrained(start_model);
    image_pipe.to(device)
    
    # Get a scheduler for sampling
    sampling_scheduler = DDIMScheduler.from_config(start_model)
    sampling_scheduler.set_timesteps(num_inference_steps=50)

    # Prepare dataset
    dataset = load_dataset(dataset_name, split="train")
    preprocess = transforms.Compose(
        [
            transforms.Resize((image_size, image_size)),
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Normalize([0.5], [0.5]),
        ]
    )
    def transform(examples):
        images = [preprocess(image.convert("RGB")) for image in examples["image"]]
        return {"images": images}
    dataset.set_transform(transform)
    train_dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)


    # Optimizer & lr scheduler
    optimizer = torch.optim.AdamW(image_pipe.unet.parameters(), lr=1e-5)
    scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.9)

    for epoch in range(num_epochs):
        for step, batch in tqdm(enumerate(train_dataloader), total=len(train_dataloader)):

            # Get the clean images
            clean_images = batch['images'].to(device)

            # Sample noise to add to the images
            noise = torch.randn(clean_images.shape).to(clean_images.device)
            bs = clean_images.shape[0]

            # Sample a random timestep for each image
            timesteps = torch.randint(0, image_pipe.scheduler.num_train_timesteps, (bs,), device=clean_images.device).long()

            # Add noise to the clean images according to the noise magnitude at each timestep
            # (this is the forward diffusion process)
            noisy_images = image_pipe.scheduler.add_noise(clean_images, noise, timesteps)

            # Get the model prediction for the noise
            noise_pred = image_pipe.unet(noisy_images, timesteps, return_dict=False)[0]

            # Compare the prediction with the actual noise:
            loss = F.mse_loss(noise_pred, noise)
            
            # Log the loss
            wandb.log({'loss':loss.item()})

            # Calculate the gradients
            loss.backward()

            # Gradient Acccumulation: Only update every grad_accumulation_steps 
            if (step+1)%grad_accumulation_steps == 0:
                optimizer.step()
                optimizer.zero_grad()
                
            # Occasionally log samples
            if (step+1)%log_samples_every == 0:
                x = torch.randn(8, 3, 256, 256).to(device) # Batch of 8
                for i, t in tqdm(enumerate(sampling_scheduler.timesteps)):
                    model_input = sampling_scheduler.scale_model_input(x, t)
                    with torch.no_grad():
                        noise_pred = image_pipe.unet(model_input, t)["sample"]
                    x = sampling_scheduler.step(noise_pred, t, x).prev_sample
                grid = torchvision.utils.make_grid(x, nrow=4)
                im = grid.permute(1, 2, 0).cpu().clip(-1, 1)*0.5 + 0.5
                im = Image.fromarray(np.array(im*255).astype(np.uint8))
                wandb.log({'Sample generations': wandb.Image(im)})
                
            # Occasionally save model
            if (step+1)%save_model_every == 0:
                image_pipe.save_pretrained(model_save_name+f'step_{step+1}')

        # Update the learning rate for the next epoch
        scheduler.step()

    # Save the pipeline one last time
    image_pipe.save_pretrained(model_save_name)
    
    # Wrap up the run
    wandb.finish()
    '''
chatgpt 解释：
这段代码是一个 Python 脚本，用于训练一个模型，该模型通过添加噪声来实现对输入图像的反向扩散，并学习去噪。以下是对代码的主要部分的解释：

引入库和模块：

代码开始时导入了各种 Python 库和模块，包括 wandb（Weights and Biases，用于记录和可视化实验）、numpy、torch 和 torchvision（用于深度学习和计算机视觉任务的库）、PIL（Python Imaging Library，用于图像处理）、tqdm（显示循环进度条的库）等。
定义命令行参数：

使用 @call_parse 装饰器，定义了一些命令行参数，用于指定模型训练的参数，如图像大小、批处理大小、训练时的累积梯度步数、训练时的周期数、预训练模型名称、数据集名称等。
初始化 WandB：

使用 wandb.init 函数初始化 Weights and Biases 以记录实验结果，并将脚本中的参数配置添加到 WandB 项目中。
准备预训练模型：

使用 DDPMPipeline.from_pretrained 函数初始化预训练模型（可能是图像去噪模型）并将其放在指定的设备上（如 GPU）。
获取采样调度器：

使用 DDIMScheduler.from_config 函数获取一个调度器，用于在训练过程中进行图像采样。
准备数据集：

使用 load_dataset 函数加载指定的数据集，然后定义数据预处理和数据转换操作，将图像调整为指定的大小、进行随机水平翻转、将图像转换为张量并进行归一化。
定义优化器和学习率调度器：

使用 AdamW 优化器和指数衰减学习率调度器。
开始模型训练循环：

通过循环遍历每个训练周期和每个数据批次，从数据中提取干净图像、添加噪声、计算模型预测、计算损失、记录损失值等。
渐变累积（Gradient Accumulation）：

为了减小GPU内存消耗，每隔一定步数（由 grad_accumulation_steps 参数指定）更新模型权重。
定期记录样本生成：

每隔一定步数（由 log_samples_every 参数指定），生成样本图像，记录并可视化在 WandB 项目中。
定期保存模型：

每隔一定步数（由 save_model_every 参数指定），保存模型的权重。
更新学习率：

每个训练周期结束后，使用学习率调度器更新学习率。
保存最终模型：

在训练结束时，保存最终的模型权重。
结束 WandB 记录：

使用 wandb.finish 结束 WandB 项目记录。
这段代码的主要目的是使用预训练模型对图像进行反向扩散，并学习去噪，同时记录和可视化训练过程和生成的样本。这是一个深度学习的训练脚本，通常用于图像处理和生成任务。
        
